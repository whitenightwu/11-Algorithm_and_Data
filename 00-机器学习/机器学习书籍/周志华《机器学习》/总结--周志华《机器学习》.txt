第一章：
数据集、特征(属性)、属性值

有监督学习分为：结果是离散的，称为分类；结果是连续的，称为回归；
二分类时，称一个为正类，另一个为负类

无监督学习：称为聚类

关于泛化能力，没有免费的午餐定理(No Free Lunch Theorem，NFL定理)
NFL定理表明没有一个学习算法可以在任何领域总是产生最准确的学习器。不管采用何种学习算法，至少存在一个目标函数，能够使得随机猜测算法是更好的算法。
平常所说的一个学习算法比另一个算法更“优越”，效果更好，只是针对特定的问题，特定的先验信息，数据的分布，训练样本的数目，代价或奖励函数等。

基于样例的学习(从训练样例中归纳出学习结果)（代表：BP）==》基于统计的学习（代表：支持向量机，核方法）==》深度学习

大数据时代的三大核心：机器学习、云计算、众包。
机器学习与数据挖掘，数据挖掘的两大支撑是机器学习（基础含有统计学）和数据库领域。

集成学习则是机器学习的首要热门方向。集成学习是使用一系列学习器进行学习，并使用某种规则把各个学习结果进行整合从而获得比单个学习器更好的学习效果的一种机器学习方法。


第二章：
A.评估方法，对数据集划分成训练集和测试集的方法：
注意：验证集（训练时用来调参和调优的）和训练集是训练数据的；另一个是测试集。
1.留出法：3/4--2/3的为训练集，剩下的为测试集
2.交叉验证法（又称交叉比对）：数据集分成多个互斥子集，每次留一个测试集，共形成k个模型。缺点：对大规模数据集太耗时；k的选取很重要
在给定的建模样本中，拿出大部分样本进行建模型，留小部分样本用刚建立的模型进行预报，并求这小部分样本的预报误差，记录它们的平方加和。
例如：10折交叉验证(10-fold cross validation)，将数据集分成十份，轮流将其中9份做训练1份做验证，10次的结果的均值作为对算法精度的估计，一般还需要进行多次10折交叉验证求均值，例如：10次10折交叉验证，以求更精确一点。
特例：留一法。k个样本分为k份

3.自助法：为克服上面两种方法没有完全利用数据集的特点（留一法除外，但留一法太耗时），该方法完全利用数据集。
对原始数据集D1放回采样，形成大小相同的新数据集D2（其中可能有元素重复，有元素没有），则D2为训练集，D1/D2为测试集。
因为改变了初始数据集的发布，该方法引入了偏差，不过对集成学习有好处；在数据集够大时，一般不用该方法

B.性能度量：衡量模型泛化能力的评价标准。
1.回归任务中一般使用MSE（均方误差）。
2.分类问题中常用：精度（acc=1-E）与错误率（E，分类错误的样本数占样本总数的比例）。

3.查准率(P，即分类后正确的多)与查全率(R，即正确的分类多)，这对是相互矛盾的（即类似反比），P-R曲线中的BEP（平衡点），P为纵轴，R为横轴。
BEP太简化了，我们用F1，它是P、R的调和平均；实际中，我们用Fb，当然还有很多其他变种。

4.P-R与ROC类似但不同！！ROC（受试者工作特征）曲线，真正例率（TPR，True Positive Rate）纵轴，假正例率（FPR）横轴，一般用AUC面积（即曲线与X轴所包围的面积）判优劣。

5.代价矩阵，代价敏感错误率，代价曲线
在非均等代价下，ROC不能直接反映学习器的期望总体代价，故使用代价曲线。横轴为正例概率代价，纵轴为归一化代价，曲线与横轴的面积是期望总体代价。
规范化又称归一化

C.比较检验：表征学习器的泛化能力（也是比较哪个学习器好）
先使用某种实验评估方法测得学习器的某个性能度量的结果，然后对结果进行比较。

1.假设检验。现实中我们不知道学习器的泛化错误率，只知道测试错误率，而两者未必相同。
二项检验，得到的结果：是以1-a的置信度认为泛化错误率不大于e0，否则该假设被拒绝（即在a的显著度下认为其泛化错误率大于e0）；
t检验，我们用留出法等方法时有很多测试错误率，这时我们得到的是一个区间

2.交叉验证t检验。我们队两个学习器使用k折交叉验证法，则若两者性能一致会有测试错误率e1=e2；具体来讲，只要计算e1-e2就可以得到学习器性能差异。但由于交叉验证法可能有不同轮次训练集会重叠的现象，我们用“5X2交叉验证”缓解这种情况。

3.McNemar检验。对于二分类问题，若性能相同则|e1-e2|服从正态分布，否则平均错误率小的学习器较优

4.Friedman检验。上面的2和3两种都是在一个数据集上比较两个算法性能；现在我们要比较多数据集比较多个算法性能，可以两两比较，也可以使用基于算法排序的Friedman检验。F检验可以知道算法性能是否相同；若不相同，我们需要“后续检验”区分各个算法，常用Nemenyi法检验（计算将得到邻域，若两算法邻域有重叠则认为没有显著差别，否则平均序值小的叫优）。

D.偏差和方差：解释学习算法的泛化性能的工具
针对回归问题：泛化误差=偏差+方差+噪声；
偏差与方差是对错误率进行拆解的一种描述。
1.偏差（bias）是期望输出与真实结果之间的偏离程度，即刻画了算法本身的拟合能力。
2.方差，同样大小数据集变化所导致的学习性能的的变化，刻画数据扰动所造成的影响。
3.噪声，在当前任务下任何学习算法所能到达的泛化误差下限，刻画的是问题本身的难度。

偏差-方差窘境：一般这两者是由冲突的。训练不足、欠拟合到完成过程中，偏差主导泛化误差；反之...


第三章:

均方误差对应了欧氏距离；是线性回归中十分重要的性能度量；
最小二乘法就是基于均方误差，找一条直线使样本到直线的欧式距离和最小；
多元线性回归也是类似，但经常找不到闭式解，故经常引入正则项；
对数线性回归：逼近仍是线性，但输入到输出是非线性映射，在这里对数函数有着将线性模型的预测值与真实标记联系的作用。

对数几率回归：考虑到二分类时，我们要将输出装换为0/1，可用单位阶跃函数（但它不连续），故使用对数几率函数（logistic function），它的图就是sigmoid。
注意，它虽叫“回归”，但事实是一种分类学习的方法
极大似然法，牛顿法，梯度下降法

线性判别分析（LDA），即fisher判别。key word：类内离散度、类间离散度、协方差矩阵、拉格朗日层子，投影。
它是二分类，可拓展到多分类中，此时叫全局离散度矩阵。LDA也是一种监督降维技术。

多分类：对于MvM，常用纠错输出编码ECOC；它事实上是一种用分类器编码的思想；

类别不平衡问题：即有的类有很多样本，有的很少。一般，我们把决策概率代表真实的概率，分类器预测概率高于观测概率就判为正。
解决的方法：再放缩的思想，即y/(1-y)*(m/n),乘以一个权重；它也是”代价敏感学习“的基础，不过公式中用cost-/cost+代替了m、n。
具体操作中主要有3种：1.对多例”欠采样“，去一些多例；2.**”过采样“；3.”阈值移动“，即决策时乘以一个权重（上面的公式）

稀疏表示、代价敏感学习、多标记学习



第四章 决策树

一个根节点（包含全集），若干内部节点（属性测试），若干叶节点（对应决策结果）
从根到每一个叶的路径对应一个判定序列，遵循简单的”分而治之“策略。
它的生成是递归过程

4.2划分选择
信息熵，度量样本集合纯度的一种指标；信息增益，越大越纯度高。
ID3决策树算法就是以信息增益为准则划分属性的

但以信息增益为准则易使叶节点纯度大、失去泛化能力。该准则对取值较多的属性有偏好。
减小偏好的影响，改用增益率划分，即C4.5决策树算法。

”基尼指数“的CART决策树

4.3剪枝处理
目的是防止过拟合，有两种不同的策略：预剪枝和后剪枝
预剪枝：从下往上生成树，对精度低的节点裁剪；剪枝的节点处可能是精度暂时下降，之后可能有好的效果，基于贪心而不准分支展开，可能带来欠拟合。
后剪枝：对生成好的决策树剪枝；效果一般更好，但训练时间开销大


贪心算法


4.4连续与缺失值
连续：
之前都是基于离散属性说决策树的。此时，连续属性离散化方法可以用上来，例如最简单的二分法

缺失值：现实中经常有不完整的样本，即某些属性缺失。
这就是两个问题。1.缺失属性时，用权重划分；2.给定属性，该样本的该属性缺失，则将其用不同的权重归到每一个子节点中

4.5多变量决策树
不是为每一个叶节点找到一个最优划分（这样开销太大），而是选择建立一个线性的分类，当然，也包括斜线，但绝不要曲线。
有OC1（贪心算法）

商业化版本C5.0



第五章
有效的神经网络是以数学证明为支持的，而生物上的只是用来类比。

M-P神经元就是一直以来使用的模型，即多输入（带不同权值）相加，经过神经元给他们同一个偏置，然后sigmoid一下，得到一个输出；
y=f（w1*x1+...+wi*xi+b）。连接权=权值；阈值=偏置。
实际上，M-P神经元模型最常用，但还有些其他模型，例如考虑了电位脉冲发放时间而不仅是积累电位的脉冲神经元模型。

sigmoid可以将较大范围变化的X，挤压到[0,1]的输出范围内。

感知机是由2层神经元组成，其中输出层是M-P神经元。也就是说，只有输出进行激活函数处理，即只有一层功能神经元。
只有一层的神经元，学习能力十分有限，它的收敛只能依靠存在一个超平面将他分开。故连非线性可分的“异或”也搞不定。

解决非线性可分问题，用多层神经元。（即隐含层和输出层都是功能神经元）
每层神经元与下层神经元全连接，神经元之间不存在跨层和同层连接；该网络结构称为“多层前馈神经网络”。
前馈不是意味着信号不能后传，而是指在网络的拓扑结构上不存在环或者回路。

BP即误差逆传播，它是基于梯度下降策略，目标是最小化训练集的积累误差E。
标准BP（每单个样例更新一次权重）和积累BP（直接针对累积误差最小化，读取整个样本集才更新即）;这两者类似于SGD和标准梯度下降。
已证明，一个包含足够多神经元的隐层，多层前馈网络就能以任意精度逼近任意连续函数；但我们怎么设隐层神经元个数呢？"试错法"调整
BP经常过拟合，有两种策略缓解：
1.“早停”，训练时分为训练集（计算梯度，更新权重、偏置）和验证集（估计误差），当训练集误差下降但验证集误差上升时停止
2.“正则化”，在误差函数中增加一项描述网络复杂度的部分（这其实与SVM非常相似），（也就是权重衰减项）如权重与偏置（又称阈值）的平方和。
这其中“入”用于对经验误差和网络复杂度折中，常用交叉验证法估计（‘入’属于（0,1））

误差目标函数也叫代价函数
网络提取规则：（1）第一步，网络剪枝，剪去对训练后的网络影响最小的加权链（2）对剪枝后的网络进行链、单元或活化值聚类。
灵敏度分析：用于评估一个给定的输入变量对网络输出的影响。改变该变量的输入，而其它输入变量固定，监测网络输出的改变。

使误差E最小，其实就是参数寻优的过程，找全局最小。基于梯度的搜索是最常用的参数寻优法，梯度下降法就是沿着负梯度方向搜索，但只能得到局部最优。
我们可以用一些其他方法跳出局部最小。
1.不同参数初始化网络，相当于不同初始点开始搜索；
2.“模拟退火”，每一步都一定概率接受比当前更差的解，而接受次优解的概率随时间逐步降低（这样可以保证算法稳定）；有时候它进入局部后一段时间会跳出来；属于贪心算法。
其第三步是判断新解是否被接受,判断的依据是一个接受准则，最常用的接受准则是Metropolis准则: 若ΔT<0则接受S′作为新的当前解S，否则以概率exp(-ΔT/T)接受S′作为新的当前解S。
3.随机梯度下降；普通和批量梯度下降（Batch）的公式中的误差是针对于所有训练样本而得到的，而随机（Stochastic）梯度下降的思想是根据每个单独的训练样本来更新权值。
批量梯度下降由于是要最小化风险函数，所以按每个参数theta的梯度负方向，来更新每个theta；它得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果m很大，那么可想而知这种方法的迭代速度！
4.遗传算法。
他们都是启发式的，理论上缺乏保证。

除BPNN外，还有许多NN
...RBF网络（径向基函数网络），他经常设置为单隐层的，用的隐层神经元激活函数是径向基函数，输出层是对隐层神经元输出的线性组合；多隐层的RBF可以以任意精度逼近任意连续函数
训练时与通常的一样：第一步，确定神经元中心，常用随机采样、聚类；第二步，BP来确定参数

...ART网络（自适应谐振理论网络），属于竞争性学习（输出神经元相互竞争，只有一个被激活，其被抑制）
分为比较层（接受输入）、识别层（其中每个神经元对应一个模式类，且元的数目可增长；竞争的最简单方式就是输入向量与元对应的模式向量之间距离最小，只有获胜者被激活，此时更新权值，使该获胜元下次有更大可能性获胜；若全部元的距离都小于识别阈值，则调用重置模块以新长出一个元）。
识别阈值对网络有重要影响，其大小直接影响分类精度；初始权值对整个算法影响重大；其优点是可增量学习或在线学习；既有学习新知识的能力，又能保持对旧知识的记忆。
具体过程中，它有两个权向量：外星权向量，竞争获胜神经元通过外星权向量返回比较层，进行相似度计算；内星权向量，对识别层所有内星权向量Bj 计算输入模式X的匹配度。
ART-1网络有两个分别的学习规则：一个用于L1-L2连接，调整的值；另一个用于L2-L1连接，调整的值当输入和期望值适当的匹配时，两者是同步更新的。匹配的过程以及随后的适应过程被称为谐振。
以上两者虽然不等，但和第一层的输出有相同的内积，称这为子集/超集二难问题。解决方法：（a）规格化（Grossberg），（b）加强中心/抑制周围。详情见blog。

...SOM（自组织映射）网络：是竞争学习型网络，且与前向神经网络不同，它是一种无监督的学习。很适宜用于数据的量化；故也称作学习向量量化器。
应用于数据聚类，数据降维。两层结构，包括输入层和竞争层。降维同时，可以保持输入数据在高维空间的拓扑结构不变。
它的基本思想是：网络输出层的各神经元通过竞争来获得对输入层的响应机会，最后只有一个神经元获胜。获胜的神经元对它临近的神经元的影响由近及远，由兴奋逐渐转为抑制，那些与获胜神经元有关的各连接权朝着有利于它竞争的方向转变。 
人话：接受样本后，计算样本向量与自身权向量的距离，距离最近的为最佳匹配单元，匹配单元与其临近单元的权向量调整，使这些权向量与当前样本的距离缩小。一个输入一次调整。
优胜邻域随训练时间收缩。学习率也会不断改变。
缺点有二：输入模式较少时，分类结果依赖于模式输入的先后次序；和ART网络不一样，SOM网络在没有经过完整的重新学习之前，不能加入新的类别。

...级联相关网络:它是结构自适应网络的代表。ATR网络也是结构自适应。
主要两部分：1.级联，初始只有两层，逐步创建新隐层神经元；注意输入端的连接权值是冻结的。2.相关，通过最大化新神经元输出与网络误差的相关性来训练参数。
与一般的前馈网络相比，它更快训练，不需要后向传播错误信号；但容易在数据太小时过拟合。
具体过程：1.候选神经元连结到所有的输入和隐含神经元，并且候选神经元的输出不连结到网络上；2.只训练候选神经元的权重；3.将候选神经元安装到图中空白的层上，这时候选项的连接权就不能再改变了；4.将候选神经元连结到网络的输出上，这时候选神经元被激活，开始训练网络的所有输出连接权；


...Elman网络是递归NN，与前馈不同。其他一致，但隐层神经元输出被反馈回来，与下一时刻的输入一起作用。通常sigmoid激活，推广的BP训练。


...boltzmann机（玻尔兹曼机）：它是基于能量的，故训练目的是使定义的“能量”最小化；分为显层和隐层两个部分；神经元都是BOOL型的；训练过程就是将每个训练样本视为一个向量，使其出现的概率尽可能大。
实际中，因为复杂度高，故用限制玻尔兹曼机（RBM，即同层神经元相互的连接去除，本来的玻尔兹曼机同层不同元会有连接），这个通常用对比散度（CD）算法训练
它不依赖于输入值的顺序；都有连接权和阈值。


DL：难以直接使用经典算法（如BP）进行训练，因为误差在多隐层中逆传播时，往往是“发散”（diverge）而不是收敛。
无监督逐层训练是多隐层网络训练的有效手段，即先逐层预训练，再对整个网络微调（思想是基于局部较优的结果联合来全局寻优）。例如：深度置信网络DBN，它每层都是受限玻尔兹曼机。
另一种节省训练开销的方法是全共享，见CNN。
深度学习中，我们将低维特征映射为高维，所以我们可以将DL理解为特征学习

主流期刊有Netural Computation；A类会议有NIPS等




第六章 支持向量机
 
支持向量：使公式成立的、离超平面最近的几个训练样本（先简单化，二分类、线性方程）
间隔：异类支持向量到超平面的距离和。我们要找的就是最大间隔的超平面，即公式中的W和b。

虽然该模型 f(x)=w^T*x+b  ，可以用现成的优化计算包求解，但傲娇不用我们将其变成“对偶问题”















