# 主动学习
一般认为，已标记的数据越多，标记越精准，基于这些数据训练得到的模型也越高效。然而，大数据提供机遇的同时也带来了严重的挑战，其中最典型的便是数据质量低下。因此如何从仅有少量标记的大数据中学习出有效模型是一个极具挑战的重要问题。
实际上，不同数据样本对于学习模型的贡献度是不一样的，如果我们能够选取一部分最有价值的数据进行标注，有可能仅基于少量数据就能获得同样高效的模型。

主动学习就是研究这一问题的一种机器学习框架。其核心任务是制定选择样本的标准（设计出合理的选择策略），从而选择尽可能少的样本进行标注来训练出一个好的学习模型。
**在我看来，主动学习就是融合、选择数据集的算法**。

## 三种主动学习场景
### 1）基于数据池的主动学习
最常见的一种场景，其假设所有未标记数据已经给定，形成一个数据池。
主动学习算法迭代进行，每一次从未标记数据池中选择样本向专家查询标记，并将这些新标注的样本加入训练集，模型基于新的训练集进行更新，进而进入下一次迭代。

### 2）基于数据流的主动学习
基于数据流的主动学习假设样本以流的形式一个一个到达，因此在某时刻当一个样本到达的时候，算法必须决定是否查询该样本的标记。
这种场景在一些实际应用中也比较常见，比如数据流源源不断产生，而又无法保存下来所有数据时，基于数据流的主动学习就更为适用。

### 3）基于合成样本查询的主动学习
不是从已有样本中选择来查询标记信息，而是直接从特征空间里合成出新的样本进行查询。
由于新合成的样本可能是特征空间里任意取值组合产生的，因此在某些应用问题中可能导致人类专家也无法标注这些合成样本。比如在图像分类任务中，任意像素取值合成的一幅图片可能并不能呈现出清晰的语义。

## 三种策略
主动学习的关键任务在于设计出合理的查询策略，即按照一定的准则来选择被查询的样本。目前的方法可以大致的分为三种策略：

### 1）基于信息量的查询策略
最为常见的，其基本思想是选择那些能最大限度减少当前模型不确定性的样本进行查询。
具体而言，**信息量又可以通过模型预测的置信度、模型错误率下降期望、委员会投票等多种形式进行度量**。这类方法选择样本时只基于现有的已标记样本，忽略了大量的未标记样本中蕴含的数据分布信息，可能导致采样偏差问题。

### 2）基于代表性的查询策略
倾向于选择那些更能刻画数据整体分布的未标记数据进行标记查询。
这些方法往往通过聚类或密度估计等无监督技术来评估样本的代表性，由于忽略了已标记样本因此整体性能也可能会依赖于聚类结果的好坏。

### 3）综合多种准则的查询策略
能同时考虑选择样本的信息量和代表性，能够有效避免采样偏差和依赖聚类结果的问题。
近年来已有研究者从不同角度提出综合多种查询准则的主动学习方法，并展示出较好的实验性能。
个人感觉，facenet中的triplet-loss就是属于这一种。


## 论外（与无监督学习的关系）
弱监督学习的目的，与监督学习一致，然而其获得的样本并没有完整的标记。从标记缺失的形式和处理方式的不同，又可以分为半监督学习、主动学习、多示例学习／多标记学习、和强化学习。

1）半监督学习中，只有少量的样本具有标记；
2）主动学习中，机器可以询问真实的标记，但需要考虑询问的代价；
3）多示例学习中，一个对象表示为一组样本的包，而标记只在包的层面上，在样本的层面上却没有标记；
4）多标记学习中，一个样本对应一组标记，因此需要处理巨大的标记组合空间问题；
5）强化学习中，机器需要探索环境来获得样本，并且学习的目的是长期的奖赏，因此样本的标记是延迟的。


# 参考资料
+++中国机器学习2015白皮书